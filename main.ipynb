{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"toc_visible":true,"mount_file_id":"1WsoOpl1EHjyA4w8lRK9fhc0MiEPXqxfF","authorship_tag":"ABX9TyOMbJrjUohozxlTcsTtPgM3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"85gRujwJGEfh","colab_type":"code","outputId":"b15ce187-01a6-4dc3-d02f-f76820603e0e","executionInfo":{"status":"ok","timestamp":1590053587891,"user_tz":-330,"elapsed":1672,"user":{"displayName":"chandan kumar Tandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-UH9fme5JqM9OJiN8K3bf7seggeRU5qDWAfNd=s64","userId":"13293660809488080635"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd drive/My\\ Drive/Colab\\ Notebooks/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G3ccMVq5VbGW","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rofsxjvL_nvp","colab_type":"code","colab":{}},"source":["import time\n","import torch\n","import numpy as np\n","from string import punctuation\n","from collections import Counter\n","from torch.utils.data import TensorDataset, DataLoader\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IDYyd_By_2w5","colab_type":"text"},"source":["**DATA PREPARATION**"]},{"cell_type":"code","metadata":{"id":"zV5HOvTkAdg-","colab_type":"code","colab":{}},"source":["log_dir=\"\"\n","writer = SummaryWriter(log_dir)\n","\n","# read data from text files\n","with open('data/reviews.txt', 'r') as f:\n","    reviews = f.read()\n","with open('data/labels.txt', 'r') as f:\n","    labels = f.read()\n","\n","#remove punctuation    \n","reviews = reviews.lower()\n","all_text = ''.join([c for c in reviews if c not in punctuation])  \n","\n","# split by new lines and spaces\n","reviews_split = all_text.split('\\n')\n","all_text = ' '.join(reviews_split)\n","\n","# create a list of words\n","words = all_text.split()  \n","\n","# word embedding and mapping\n","counts = Counter(words)\n","vocab = sorted(counts, key=counts.get, reverse=True)\n","vocab_to_int = {word: ii for ii, word in enumerate(vocab,1)}\n","\n","#tokenize\n","reviews_ints = []\n","for review in reviews_split:\n","  reviews_ints.append([vocab_to_int[word] for word in review.split()])\n","  \n","# 1=positive, 0=negative label conversion\n","labels_split = labels.split('\\n')\n","encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])\n","\n","# review length\n","review_lens = Counter([len(x) for x in reviews_ints])\n","\n","\n","# remove 0-length review with their labels\n","non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n","\n","reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n","encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx], dtype=np.int64)\n","\n","\n","def pad_features(reviews_ints, seq_length):\n","    ''' Return features of review_ints, where each review is padded with 0's \n","        or truncated to the input seq_length.\n","    '''\n","    ## getting the correct rows x cols shape\n","    features = np.zeros((len(reviews_ints), seq_length), dtype=np.int64)\n","    \n","    ## for each review, I grab that review\n","    for i, row in enumerate(reviews_ints):\n","      features[i, -len(row):] = np.array(row)[:seq_length]\n","    \n","    return features\n","\n","seq_length = 200\n","\n","features = pad_features(reviews_ints, seq_length=seq_length)\n","\n","# train test split\n","split_frac = 0.8\n","\n","## split data into training, validation, and test data (features and labels, x and y)\n","split_idx = int(len(features)*0.8)\n","train_x, remaining_x = features[:split_idx], features[split_idx:]\n","train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n","\n","test_idx = int(len(remaining_x)*0.5)\n","val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n","val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cycN_6U-JVTD","colab_type":"text"},"source":["**DATA LOAD**"]},{"cell_type":"code","metadata":{"id":"Hpw5ckEKJZbf","colab_type":"code","colab":{}},"source":["# create Tensor datasets\n","train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n","\n","# dataloaders\n","batch_size = 50\n","\n","# make sure to SHUFFLE your data\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjV-Vna9HCv_","colab_type":"text"},"source":["**LSTM Model**"]},{"cell_type":"code","metadata":{"id":"3Mw4JzaPHLnr","colab_type":"code","colab":{}},"source":["class ReviewLSTM(nn.Module):\n","    \"\"\"\n","    The LSTM model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n","        \"\"\"\n","        Initialize the model by setting up the layers.\n","        \"\"\"\n","        super(ReviewLSTM, self).__init__()\n","\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        # embedding and LSTM layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n","                            dropout=drop_prob, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","        \n","        # linear and sigmoid layer\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size = x.size(0)\n","        \n","        # embeddings and lstm_out\n","        embeds = self.embedding(x)\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","        \n","        # stack up lstm outputs\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        # dropout and fully connected layer\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        \n","        # sigmoid function\n","        sig_out = self.sig(out)\n","        \n","        # reshape to be batch_size first\n","        sig_out = sig_out.view(batch_size, -1)\n","        sig_out = sig_out[:, -1] # get last batch of labels\n","        \n","        # return last sigmoid output and hidden state\n","        return sig_out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","\n","        \n","        return hidden"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJKO0p9kH0zo","colab_type":"text"},"source":["**TRAINING**"]},{"cell_type":"code","metadata":{"id":"AWHBYTf0H-XP","colab_type":"code","outputId":"c58c7f83-c194-4b25-9265-c50c463b8e8b","executionInfo":{"status":"ok","timestamp":1590057825766,"user_tz":-330,"elapsed":980748,"user":{"displayName":"chandan kumar Tandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-UH9fme5JqM9OJiN8K3bf7seggeRU5qDWAfNd=s64","userId":"13293660809488080635"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["vocab_size = len(vocab_to_int) + 1 # +1 for zero padding + our word tokens\n","output_size = 1\n","embedding_dim = 400 \n","hidden_dim = 256\n","n_layers = 2\n","\n","model = ReviewLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","model.cuda()\n","model.train()\n","        \n","lr=0.0001\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","epochs = 17 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 100\n","clip=5 # gradient clipping\n","\n","\n","for e in range(epochs):\n","\n","    #time calculation\n","    start = time.time()\n","\n","    #initialize total training loss\n","    total_training_loss= 0.0\n","\n","    # initialize hidden state\n","    h = model.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","            \n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        model.zero_grad()\n","\n","        # get the output from the model\n","        output, h = model(inputs, h)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        total_training_loss+=loss.item()\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = model.init_hidden(batch_size)\n","            val_losses = []\n","            model.eval()\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","\n","                inputs, labels = inputs.cuda(), labels.cuda()\n","                    \n","\n","                output, val_h = model(inputs, val_h)\n","                val_loss = criterion(output.squeeze(), labels.float())\n","\n","                val_losses.append(val_loss.item())\n","\n","            model.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n","            writer.add_scalar('Validation_Loss',np.mean(val_losses),counter)\n","    end = time.time()\n","    writer.add_scalar('Time_Taken_for_each_epoch',end-start,e)\n","    writer.add_scalar('Total_training_loss_for_each_epoch', total_training_loss, e)\n","            \n","# Get test data loss and accuracy\n","\n","test_losses = [] # track loss\n","num_correct = 0\n","\n","# init hidden state\n","h = model.init_hidden(batch_size)\n","\n","model.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    h = tuple([each.data for each in h])\n","\n","    inputs, labels = inputs.cuda(), labels.cuda()\n","        \n","    \n","    # get predicted outputs\n","    output, h = model(inputs, h)\n","    \n","    # calculate loss\n","    test_loss = criterion(output.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}\".format(test_acc))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch: 1/17... Step: 100... Loss: 0.694316... Val Loss: 0.692038\n","Epoch: 1/17... Step: 200... Loss: 0.686400... Val Loss: 0.680889\n","Epoch: 1/17... Step: 300... Loss: 0.583242... Val Loss: 0.596320\n","Epoch: 1/17... Step: 400... Loss: 0.638035... Val Loss: 0.561320\n","Epoch: 2/17... Step: 500... Loss: 0.420604... Val Loss: 0.569542\n","Epoch: 2/17... Step: 600... Loss: 0.567373... Val Loss: 0.502231\n","Epoch: 2/17... Step: 700... Loss: 0.456642... Val Loss: 0.451310\n","Epoch: 2/17... Step: 800... Loss: 0.416352... Val Loss: 0.446645\n","Epoch: 3/17... Step: 900... Loss: 0.358730... Val Loss: 0.492935\n","Epoch: 3/17... Step: 1000... Loss: 0.530183... Val Loss: 0.452473\n","Epoch: 3/17... Step: 1100... Loss: 0.383606... Val Loss: 0.477211\n","Epoch: 3/17... Step: 1200... Loss: 0.334373... Val Loss: 0.427914\n","Epoch: 4/17... Step: 1300... Loss: 0.268622... Val Loss: 0.455816\n","Epoch: 4/17... Step: 1400... Loss: 0.438608... Val Loss: 0.513695\n","Epoch: 4/17... Step: 1500... Loss: 0.330515... Val Loss: 0.410962\n","Epoch: 4/17... Step: 1600... Loss: 0.337100... Val Loss: 0.406487\n","Epoch: 5/17... Step: 1700... Loss: 0.240488... Val Loss: 0.465649\n","Epoch: 5/17... Step: 1800... Loss: 0.297727... Val Loss: 0.421929\n","Epoch: 5/17... Step: 1900... Loss: 0.238587... Val Loss: 0.422818\n","Epoch: 5/17... Step: 2000... Loss: 0.232484... Val Loss: 0.443328\n","Epoch: 6/17... Step: 2100... Loss: 0.228241... Val Loss: 0.519701\n","Epoch: 6/17... Step: 2200... Loss: 0.267504... Val Loss: 0.445441\n","Epoch: 6/17... Step: 2300... Loss: 0.265920... Val Loss: 0.444425\n","Epoch: 6/17... Step: 2400... Loss: 0.253739... Val Loss: 0.421769\n","Epoch: 7/17... Step: 2500... Loss: 0.166137... Val Loss: 0.499701\n","Epoch: 7/17... Step: 2600... Loss: 0.148315... Val Loss: 0.511745\n","Epoch: 7/17... Step: 2700... Loss: 0.134187... Val Loss: 0.464841\n","Epoch: 7/17... Step: 2800... Loss: 0.213206... Val Loss: 0.533033\n","Epoch: 8/17... Step: 2900... Loss: 0.208493... Val Loss: 0.594355\n","Epoch: 8/17... Step: 3000... Loss: 0.158416... Val Loss: 0.605663\n","Epoch: 8/17... Step: 3100... Loss: 0.269951... Val Loss: 0.608224\n","Epoch: 8/17... Step: 3200... Loss: 0.151384... Val Loss: 0.488543\n","Epoch: 9/17... Step: 3300... Loss: 0.120234... Val Loss: 0.537686\n","Epoch: 9/17... Step: 3400... Loss: 0.273544... Val Loss: 0.508144\n","Epoch: 9/17... Step: 3500... Loss: 0.260220... Val Loss: 0.604231\n","Epoch: 9/17... Step: 3600... Loss: 0.052641... Val Loss: 0.582492\n","Epoch: 10/17... Step: 3700... Loss: 0.110738... Val Loss: 0.586631\n","Epoch: 10/17... Step: 3800... Loss: 0.086126... Val Loss: 0.644527\n","Epoch: 10/17... Step: 3900... Loss: 0.042908... Val Loss: 0.626866\n","Epoch: 10/17... Step: 4000... Loss: 0.015783... Val Loss: 0.635226\n","Epoch: 11/17... Step: 4100... Loss: 0.162337... Val Loss: 0.669522\n","Epoch: 11/17... Step: 4200... Loss: 0.126952... Val Loss: 0.696138\n","Epoch: 11/17... Step: 4300... Loss: 0.170707... Val Loss: 0.667310\n","Epoch: 11/17... Step: 4400... Loss: 0.015875... Val Loss: 0.638664\n","Epoch: 12/17... Step: 4500... Loss: 0.064259... Val Loss: 0.745273\n","Epoch: 12/17... Step: 4600... Loss: 0.032345... Val Loss: 0.732209\n","Epoch: 12/17... Step: 4700... Loss: 0.027360... Val Loss: 0.721366\n","Epoch: 12/17... Step: 4800... Loss: 0.017472... Val Loss: 0.703364\n","Epoch: 13/17... Step: 4900... Loss: 0.028204... Val Loss: 0.761938\n","Epoch: 13/17... Step: 5000... Loss: 0.268596... Val Loss: 0.804428\n","Epoch: 13/17... Step: 5100... Loss: 0.125436... Val Loss: 0.767887\n","Epoch: 13/17... Step: 5200... Loss: 0.151849... Val Loss: 0.779564\n","Epoch: 14/17... Step: 5300... Loss: 0.080079... Val Loss: 0.752401\n","Epoch: 14/17... Step: 5400... Loss: 0.009071... Val Loss: 0.735283\n","Epoch: 14/17... Step: 5500... Loss: 0.050245... Val Loss: 0.834601\n","Epoch: 14/17... Step: 5600... Loss: 0.058614... Val Loss: 0.812538\n","Epoch: 15/17... Step: 5700... Loss: 0.004135... Val Loss: 0.902418\n","Epoch: 15/17... Step: 5800... Loss: 0.020545... Val Loss: 0.796937\n","Epoch: 15/17... Step: 5900... Loss: 0.019951... Val Loss: 0.939342\n","Epoch: 15/17... Step: 6000... Loss: 0.002107... Val Loss: 0.903090\n","Epoch: 16/17... Step: 6100... Loss: 0.004436... Val Loss: 0.913262\n","Epoch: 16/17... Step: 6200... Loss: 0.015183... Val Loss: 0.854651\n","Epoch: 16/17... Step: 6300... Loss: 0.002784... Val Loss: 0.824705\n","Epoch: 16/17... Step: 6400... Loss: 0.004062... Val Loss: 0.852301\n","Epoch: 17/17... Step: 6500... Loss: 0.003169... Val Loss: 0.829359\n","Epoch: 17/17... Step: 6600... Loss: 0.024556... Val Loss: 0.862546\n","Epoch: 17/17... Step: 6700... Loss: 0.001787... Val Loss: 0.837711\n","Epoch: 17/17... Step: 6800... Loss: 0.002354... Val Loss: 0.880918\n","Test loss: 0.906\n","Test accuracy: 0.804\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"muDqR1JTKAUf","colab_type":"text"},"source":["**RAW TESTING**"]},{"cell_type":"code","metadata":{"id":"U_EZ0b1yKFMY","colab_type":"code","colab":{}},"source":["test_review_neg = 'Nice  make one in every State of India.  Do in whole India  state wise.'\n","\n","def tokenize_review(test_review):\n","    test_review = test_review.lower() # lowercase\n","    # get rid of punctuatuon\n","    test_text = ''.join([c for c in test_review if c not in punctuation])\n","    \n","    # splitting by spaces\n","    test_words = test_text.split()\n","    \n","    # tokens\n","    test_ints = []\n","    test_ints.append([vocab_to_int[word] for word in test_words])\n","    \n","    return test_ints\n","  \n","# test code and generate tokenized review\n","test_ints = tokenize_review(test_review_neg)\n","\n","seq_length = 200\n","features = pad_features(test_ints, seq_length)\n","\n","feature_tensor = torch.from_numpy(features)\n","\n","def predict(net, test_review, sequence_length=200):\n","    ''' Prints out whether a give review is predicted to be \n","        positive or negative in sentiment, using a trained model.\n","        \n","        params:\n","        net - A trained net \n","        test_review - a review made of normal text and punctuation\n","        sequence_length - the padded length of a review\n","        '''\n","    \n","    net.eval()\n","    \n","    # tokenize review\n","    test_ints = tokenize_review(test_review)\n","    \n","    # pad tokenize sequence\n","    seq_length = sequence_length\n","    features = pad_features(test_ints, seq_length)\n","    \n","    # convert to tensor to pass to model\n","    feature_tensor = torch.from_numpy(features)\n","    \n","    batch_size = feature_tensor.size(0)\n","    \n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","    \n","    feature_tensor = feature_tensor.cuda() \n","      \n","    # get the output from the model\n","    output, h = net(feature_tensor, h)\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())\n","    # printing output value, before rounding\n","    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n","    \n","    # print custom response based on whether test_review is pos/neg\n","    if(pred.item()==1):\n","      print('Positive review detected!')\n","    else:\n","      print('Negative review detected!')\n","\n","      \n","test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'\n","\n","seq_length=200      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tbw_tRlLKt_Y","colab_type":"text"},"source":["**RESULT**"]},{"cell_type":"code","metadata":{"id":"uuG8KAzgKyD3","colab_type":"code","outputId":"71ac9e54-1a57-48d8-f077-7fdc68e0ceb9","executionInfo":{"status":"ok","timestamp":1583671379769,"user_tz":-330,"elapsed":1360,"user":{"displayName":"chandan kumar Tandi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-UH9fme5JqM9OJiN8K3bf7seggeRU5qDWAfNd=s64","userId":"13293660809488080635"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["predict(model, test_review_neg, seq_length)\n","predict(model, test_review_pos, seq_length)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Prediction value, pre-rounding: 0.135852\n","Negative review detected!\n","Prediction value, pre-rounding: 0.646279\n","Positive review detected!\n"],"name":"stdout"}]}]}
